{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Asynchronous Programming"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Non-blocking I/O"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "%pip install asyncio aiohttp nest_asyncio numpy -q\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching data from source 1...\n",
      "Fetching data from source 2...\n",
      "Fetching data from source 3...\n",
      "Processing data: Data from source 1\n",
      "Processing data: Data from source 2\n",
      "Processing data: Data from source 3\n",
      "Final processed data: ['Processed: Data from source 1', 'Processed: Data from source 2', 'Processed: Data from source 3']\n"
     ]
    }
   ],
   "source": [
    "import asyncio\n",
    "import nest_asyncio\n",
    "\n",
    "# Patch the event loop to allow nested asyncio calls in Jupyter Notebook environments\n",
    "nest_asyncio.apply()\n",
    "\n",
    "async def fetch_data(source_id):\n",
    "    \"\"\"\n",
    "    Simulates an asynchronous I/O-bound operation to fetch data from a given source.\n",
    "\n",
    "    Args:\n",
    "        source_id (int): The identifier of the data source (e.g., API, file).\n",
    "    \n",
    "    Returns:\n",
    "        str: Simulated data from the specified source.\n",
    "    \n",
    "    Note:\n",
    "        The function simulates a delay (e.g., network request or file read)\n",
    "        using `await asyncio.sleep(2)` to represent a non-blocking I/O-bound task.\n",
    "    \"\"\"\n",
    "    print(f\"Fetching data from source {source_id}...\")\n",
    "    await asyncio.sleep(2)  # Simulate I/O delay such as network call or file read.\n",
    "    return f\"Data from source {source_id}\"\n",
    "\n",
    "async def process_data(data):\n",
    "    \"\"\"\n",
    "    Simulates an asynchronous processing operation on the fetched data.\n",
    "\n",
    "    Args:\n",
    "        data (str): The data to be processed.\n",
    "    \n",
    "    Returns:\n",
    "        str: Processed version of the input data.\n",
    "    \n",
    "    Note:\n",
    "        The function simulates a processing delay (e.g., CPU-bound computation)\n",
    "        using `await asyncio.sleep(1)` to represent non-blocking processing time.\n",
    "    \"\"\"\n",
    "    print(f\"Processing data: {data}\")\n",
    "    await asyncio.sleep(1)  # Simulate data processing delay (e.g., computations).\n",
    "    return f\"Processed: {data}\"\n",
    "\n",
    "async def main():\n",
    "    \"\"\"\n",
    "    The main asynchronous function that manages data fetching and processing concurrently.\n",
    "    \n",
    "    Steps:\n",
    "        1. Fetch data concurrently from multiple sources.\n",
    "        2. Process the fetched data concurrently.\n",
    "    \n",
    "    Returns:\n",
    "        list: A list of processed data from all sources.\n",
    "    \n",
    "    Note:\n",
    "        The `asyncio.gather` function is used to run the `fetch_data` and `process_data`\n",
    "        tasks concurrently, making efficient use of I/O-bound operations.\n",
    "    \"\"\"\n",
    "    sources = [1, 2, 3]  # Simulated data sources (e.g., files, APIs, databases)\n",
    "    \n",
    "    # Step 1: Fetch data concurrently from all sources\n",
    "    fetch_tasks = [fetch_data(source) for source in sources]\n",
    "    raw_data = await asyncio.gather(*fetch_tasks)  # Wait for all fetch tasks to complete\n",
    "    \n",
    "    # Step 2: Process the fetched data concurrently\n",
    "    process_tasks = [process_data(data) for data in raw_data]\n",
    "    processed_data = await asyncio.gather(*process_tasks)  # Wait for all processing tasks to complete\n",
    "    \n",
    "    return processed_data\n",
    "\n",
    "# In a Jupyter environment, we can directly await the main function\n",
    "# This will execute the entire process of fetching and processing data concurrently\n",
    "results = await main()\n",
    "print(\"Final processed data:\", results)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Event-driven architectures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed: Event 0\n",
      "Processed: Event 1\n",
      "Processed: Event 2\n",
      "Processed: Event 3\n",
      "Processed: Event 4\n"
     ]
    }
   ],
   "source": [
    "import asyncio\n",
    "\n",
    "class AIEventProcessor:\n",
    "    \"\"\"\n",
    "    Simulates an AI processor that handles individual events asynchronously.\n",
    "\n",
    "    Methods:\n",
    "        process_event(event: str): Simulates the asynchronous processing of an event.\n",
    "    \"\"\"\n",
    "    async def process_event(self, event):\n",
    "        \"\"\"\n",
    "        Asynchronously processes an event, simulating AI-based event handling.\n",
    "\n",
    "        Args:\n",
    "            event (str): The event to be processed.\n",
    "\n",
    "        Returns:\n",
    "            str: A string indicating the result of the processing.\n",
    "        \n",
    "        Note:\n",
    "            This function includes a simulated delay (`await asyncio.sleep(1)`)\n",
    "            to mimic real-world AI computation or data analysis time.\n",
    "        \"\"\"\n",
    "        await asyncio.sleep(1)  # Simulate a 1-second AI processing delay\n",
    "        return f\"Processed: {event}\"  # Return the result after processing\n",
    "\n",
    "class EventDrivenAISystem:\n",
    "    \"\"\"\n",
    "    Manages an event-driven AI system that produces and consumes events asynchronously.\n",
    "\n",
    "    Attributes:\n",
    "        processor (AIEventProcessor): An instance of AIEventProcessor to process events.\n",
    "        queue (asyncio.Queue): An asynchronous queue to hold events for processing.\n",
    "    \n",
    "    Methods:\n",
    "        produce_event(event: str): Asynchronously adds an event to the queue.\n",
    "        consume_events(): Continuously consumes and processes events from the queue.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        Initializes the EventDrivenAISystem with an AI event processor and an event queue.\n",
    "        \"\"\"\n",
    "        self.processor = AIEventProcessor()  # Initialize the AI event processor\n",
    "        self.queue = asyncio.Queue()  # Create an asyncio queue to store events\n",
    "\n",
    "    async def produce_event(self, event):\n",
    "        \"\"\"\n",
    "        Adds an event to the queue asynchronously.\n",
    "\n",
    "        Args:\n",
    "            event (str): The event to be added to the queue.\n",
    "        \n",
    "        Note:\n",
    "            This method simulates the event production process in an asynchronous\n",
    "            environment, where events are dynamically added to a queue for processing.\n",
    "        \"\"\"\n",
    "        await self.queue.put(event)  # Add the event to the queue\n",
    "\n",
    "    async def consume_events(self):\n",
    "        \"\"\"\n",
    "        Continuously consumes and processes events from the queue asynchronously.\n",
    "\n",
    "        Note:\n",
    "            This method runs indefinitely and processes events from the queue as they\n",
    "            arrive. Once an event is processed, it marks the task as done using `task_done()`.\n",
    "        \"\"\"\n",
    "        while True:\n",
    "            event = await self.queue.get()  # Wait for an event to be available in the queue\n",
    "            result = await self.processor.process_event(event)  # Process the event using AI processor\n",
    "            print(result)  # Output the result of the processed event\n",
    "            self.queue.task_done()  # Mark the task as done to indicate completion\n",
    "\n",
    "async def main():\n",
    "    \"\"\"\n",
    "    Main coroutine that sets up the event-driven AI system, produces events,\n",
    "    and consumes them concurrently.\n",
    "    \n",
    "    Steps:\n",
    "        1. Initialize the EventDrivenAISystem.\n",
    "        2. Start the event consumer task.\n",
    "        3. Produce several events.\n",
    "        4. Wait for the queue to be processed.\n",
    "        5. Cancel the consumer task once all events are processed.\n",
    "    \n",
    "    Note:\n",
    "        The consumer task runs indefinitely in the background and is explicitly\n",
    "        cancelled after all events in the queue are processed.\n",
    "    \"\"\"\n",
    "    system = EventDrivenAISystem()  # Initialize the event-driven AI system\n",
    "    consumer = asyncio.create_task(system.consume_events())  # Start consuming events asynchronously\n",
    "    \n",
    "    # Step 1: Simulate event production (adding events to the queue)\n",
    "    for i in range(5):  # Simulate producing 5 events\n",
    "        await system.produce_event(f\"Event {i}\")\n",
    "    \n",
    "    # Step 2: Wait until all events are processed and the queue is empty\n",
    "    await system.queue.join()\n",
    "    \n",
    "    # Step 3: Cancel the consumer task once all events have been processed\n",
    "    consumer.cancel()\n",
    "\n",
    "# Run the main asynchronous event loop\n",
    "asyncio.run(main())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipeline parallelism"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessed: Data1\n",
      "Preprocessed: Data2\n",
      "Preprocessed: Data3\n",
      "Preprocessed: Data4\n",
      "Preprocessed: Data5\n",
      "Inference: Preprocessed: Data1\n",
      "Inference: Preprocessed: Data2\n",
      "Inference: Preprocessed: Data3\n",
      "Inference: Preprocessed: Data4\n",
      "Inference: Preprocessed: Data5\n",
      "Postprocessed: Inference: Preprocessed: Data1\n",
      "Postprocessed: Inference: Preprocessed: Data2\n",
      "Postprocessed: Inference: Preprocessed: Data3\n",
      "Postprocessed: Inference: Preprocessed: Data4\n",
      "Postprocessed: Inference: Preprocessed: Data5\n",
      "Final result: Postprocessed: Inference: Preprocessed: Data1\n",
      "Final result: Postprocessed: Inference: Preprocessed: Data2\n",
      "Final result: Postprocessed: Inference: Preprocessed: Data3\n",
      "Final result: Postprocessed: Inference: Preprocessed: Data4\n",
      "Final result: Postprocessed: Inference: Preprocessed: Data5\n"
     ]
    }
   ],
   "source": [
    "import asyncio\n",
    "from typing import List\n",
    "\n",
    "class AsyncAIPipeline:\n",
    "    \"\"\"\n",
    "    A class to represent an asynchronous AI processing pipeline.\n",
    "    \n",
    "    The pipeline performs three stages for each item: preprocessing, model inference, \n",
    "    and postprocessing, all in a non-blocking, asynchronous manner.\n",
    "    \"\"\"\n",
    "    \n",
    "    async def preprocess(self, data: str) -> str:\n",
    "        \"\"\"\n",
    "        Simulates the preprocessing step of the pipeline.\n",
    "\n",
    "        Args:\n",
    "            data (str): The raw input data.\n",
    "\n",
    "        Returns:\n",
    "            str: The preprocessed data.\n",
    "        \"\"\"\n",
    "        await asyncio.sleep(0.1)  # Simulate a brief delay for preprocessing\n",
    "        preprocessed_data = f\"Preprocessed: {data}\"\n",
    "        print(preprocessed_data)  # Print preprocessing stage\n",
    "        return preprocessed_data\n",
    "\n",
    "    async def model_inference(self, data: str) -> str:\n",
    "        \"\"\"\n",
    "        Simulates the model inference step of the pipeline.\n",
    "\n",
    "        Args:\n",
    "            data (str): The preprocessed data.\n",
    "\n",
    "        Returns:\n",
    "            str: The inference result.\n",
    "        \"\"\"\n",
    "        await asyncio.sleep(0.2)  # Simulate a slightly longer delay for model inference\n",
    "        inference_result = f\"Inference: {data}\"\n",
    "        print(inference_result)  # Print inference stage\n",
    "        return inference_result\n",
    "\n",
    "    async def postprocess(self, data: str) -> str:\n",
    "        \"\"\"\n",
    "        Simulates the postprocessing step of the pipeline.\n",
    "\n",
    "        Args:\n",
    "            data (str): The data after model inference.\n",
    "\n",
    "        Returns:\n",
    "            str: The postprocessed result.\n",
    "        \"\"\"\n",
    "        await asyncio.sleep(0.1)  # Simulate a brief delay for postprocessing\n",
    "        postprocessed_data = f\"Postprocessed: {data}\"\n",
    "        print(postprocessed_data)  # Print postprocessing stage\n",
    "        return postprocessed_data\n",
    "\n",
    "    async def process_item(self, item: str) -> str:\n",
    "        \"\"\"\n",
    "        Processes an individual item through the full pipeline:\n",
    "        preprocessing, model inference, and postprocessing.\n",
    "\n",
    "        Args:\n",
    "            item (str): The raw input data.\n",
    "\n",
    "        Returns:\n",
    "            str: The fully processed result.\n",
    "        \"\"\"\n",
    "        # Preprocess the input data\n",
    "        preprocessed = await self.preprocess(item)\n",
    "        \n",
    "        # Perform model inference on the preprocessed data\n",
    "        inference_result = await self.model_inference(preprocessed)\n",
    "        \n",
    "        # Postprocess the inference result\n",
    "        postprocessed = await self.postprocess(inference_result)\n",
    "        \n",
    "        return postprocessed\n",
    "\n",
    "    async def run_pipeline(self, items: List[str]) -> List[str]:\n",
    "        \"\"\"\n",
    "        Runs the pipeline for a list of items concurrently.\n",
    "\n",
    "        Args:\n",
    "            items (List[str]): A list of raw input data items.\n",
    "\n",
    "        Returns:\n",
    "            List[str]: A list of fully processed results.\n",
    "        \"\"\"\n",
    "        # Create asynchronous tasks for each item in the list\n",
    "        tasks = [self.process_item(item) for item in items]\n",
    "        \n",
    "        # Execute all tasks concurrently and return the results\n",
    "        return await asyncio.gather(*tasks)\n",
    "\n",
    "async def main():\n",
    "    \"\"\"\n",
    "    Main function to demonstrate running the asynchronous AI pipeline.\n",
    "    \"\"\"\n",
    "    # Initialize the AI processing pipeline\n",
    "    pipeline = AsyncAIPipeline()\n",
    "    \n",
    "    # Sample input data\n",
    "    input_data = [\"Data1\", \"Data2\", \"Data3\", \"Data4\", \"Data5\"]\n",
    "    \n",
    "    # Run the pipeline on the input data and gather the results\n",
    "    results = await pipeline.run_pipeline(input_data)\n",
    "    \n",
    "    # Print each result to the console\n",
    "    for result in results:\n",
    "        print(f\"Final result: {result}\")\n",
    "\n",
    "# Start the event loop and run the main function\n",
    "asyncio.run(main())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Asynchronous Parameter Updates in Distributed AI Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Worker 0 completed iteration 1\n",
      "Worker 2 completed iteration 1\n",
      "Worker 0 completed iteration 2\n",
      "Worker 1 completed iteration 1\n",
      "Worker 2 completed iteration 2\n",
      "Worker 0 completed iteration 3\n",
      "Worker 0 completed iteration 4\n",
      "Worker 1 completed iteration 2\n",
      "Worker 0 completed iteration 5\n",
      "Worker 2 completed iteration 3\n",
      "Worker 1 completed iteration 3\n",
      "Worker 0 completed iteration 6\n",
      "Worker 2 completed iteration 4\n",
      "Worker 1 completed iteration 4\n",
      "Worker 0 completed iteration 7\n",
      "Worker 2 completed iteration 5\n",
      "Worker 1 completed iteration 5\n",
      "Worker 2 completed iteration 6\n",
      "Worker 1 completed iteration 6\n",
      "Worker 0 completed iteration 8\n",
      "Worker 1 completed iteration 7\n",
      "Worker 2 completed iteration 7\n",
      "Worker 1 completed iteration 8\n",
      "Worker 0 completed iteration 9\n",
      "Worker 1 completed iteration 9\n",
      "Worker 0 completed iteration 10\n",
      "Worker 2 completed iteration 8\n",
      "Worker 1 completed iteration 10\n",
      "Worker 2 completed iteration 9\n",
      "Worker 2 completed iteration 10\n",
      "Final parameters: [-0.03253866 -0.07696249 -0.01870058  0.04505101  0.06242029]\n"
     ]
    }
   ],
   "source": [
    "import asyncio\n",
    "import numpy as np\n",
    "from typing import List\n",
    "\n",
    "class ParameterServer:\n",
    "    \"\"\"\n",
    "    A class that simulates a parameter server to store and update parameters \n",
    "    for a distributed training system.\n",
    "    \n",
    "    Attributes:\n",
    "        params (np.ndarray): An array of parameters initialized to zeros.\n",
    "        lock (asyncio.Lock): A lock to ensure synchronous updates to the parameters.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, param_size: int):\n",
    "        \"\"\"\n",
    "        Initializes the parameter server with a parameter array of given size.\n",
    "        \n",
    "        Args:\n",
    "            param_size (int): The size of the parameter array.\n",
    "        \"\"\"\n",
    "        self.params = np.zeros(param_size)  # Initialize parameters to zeros\n",
    "        self.lock = asyncio.Lock()  # Lock to handle concurrent access to parameters\n",
    "\n",
    "    async def get_params(self) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Asynchronously fetches a copy of the parameters.\n",
    "        \n",
    "        Returns:\n",
    "            np.ndarray: A copy of the current parameters.\n",
    "        \"\"\"\n",
    "        async with self.lock:  # Ensures synchronous access to parameters\n",
    "            return self.params.copy()  # Return a copy of the parameters\n",
    "\n",
    "    async def update_params(self, gradients: np.ndarray, learning_rate: float = 0.01):\n",
    "        \"\"\"\n",
    "        Asynchronously updates the parameters based on gradients using gradient descent.\n",
    "        \n",
    "        Args:\n",
    "            gradients (np.ndarray): The computed gradients to update the parameters.\n",
    "            learning_rate (float): The learning rate for parameter update (default is 0.01).\n",
    "        \"\"\"\n",
    "        async with self.lock:  # Lock ensures only one update happens at a time\n",
    "            self.params -= learning_rate * gradients  # Update parameters using gradient descent\n",
    "\n",
    "class Worker:\n",
    "    \"\"\"\n",
    "    A class representing a worker in a distributed training system.\n",
    "    \n",
    "    Attributes:\n",
    "        worker_id (int): The unique identifier for the worker.\n",
    "        param_server (ParameterServer): The parameter server instance to fetch and update parameters.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, worker_id: int, param_server: ParameterServer):\n",
    "        \"\"\"\n",
    "        Initializes a worker with an ID and a reference to the parameter server.\n",
    "        \n",
    "        Args:\n",
    "            worker_id (int): The unique identifier for the worker.\n",
    "            param_server (ParameterServer): The parameter server instance.\n",
    "        \"\"\"\n",
    "        self.worker_id = worker_id  # Worker ID for tracking\n",
    "        self.param_server = param_server  # Parameter server to fetch/update parameters\n",
    "\n",
    "    async def compute_gradients(self, params: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Simulates gradient computation asynchronously by generating random gradients.\n",
    "        \n",
    "        Args:\n",
    "            params (np.ndarray): The current parameters.\n",
    "        \n",
    "        Returns:\n",
    "            np.ndarray: Simulated gradients (random values with the same shape as parameters).\n",
    "        \"\"\"\n",
    "        # Simulate gradient computation with random delay (I/O-bound task)\n",
    "        await asyncio.sleep(np.random.rand())  # Random sleep time to simulate computation\n",
    "        return np.random.randn(*params.shape)  # Return random gradients of the same shape as params\n",
    "\n",
    "    async def train(self, num_iterations: int):\n",
    "        \"\"\"\n",
    "        Simulates the training process for a specified number of iterations.\n",
    "        \n",
    "        Args:\n",
    "            num_iterations (int): The number of training iterations to perform.\n",
    "        \"\"\"\n",
    "        for i in range(num_iterations):\n",
    "            # Fetch the current parameters from the parameter server\n",
    "            params = await self.param_server.get_params()\n",
    "            # Compute gradients based on the fetched parameters\n",
    "            gradients = await self.compute_gradients(params)\n",
    "            # Update parameters on the server based on computed gradients\n",
    "            await self.param_server.update_params(gradients)\n",
    "            # Log the completion of each iteration\n",
    "            print(f\"Worker {self.worker_id} completed iteration {i+1}\")\n",
    "\n",
    "async def distributed_training(num_workers: int, param_size: int, num_iterations: int):\n",
    "    \"\"\"\n",
    "    Simulates distributed training with multiple workers interacting with a central parameter server.\n",
    "    \n",
    "    Args:\n",
    "        num_workers (int): The number of workers to participate in the training.\n",
    "        param_size (int): The size of the parameter array managed by the parameter server.\n",
    "        num_iterations (int): The number of training iterations each worker will perform.\n",
    "    \"\"\"\n",
    "    # Create a parameter server with the specified parameter size\n",
    "    param_server = ParameterServer(param_size)\n",
    "    \n",
    "    # Initialize workers and assign each to the parameter server\n",
    "    workers = [Worker(i, param_server) for i in range(num_workers)]\n",
    "    \n",
    "    # Create asynchronous training tasks for all workers\n",
    "    tasks = [worker.train(num_iterations) for worker in workers]\n",
    "    \n",
    "    # Run all worker training tasks concurrently\n",
    "    await asyncio.gather(*tasks)\n",
    "    \n",
    "    # Fetch and print the final parameters from the parameter server after training\n",
    "    final_params = await param_server.get_params()\n",
    "    print(f\"Final parameters: {final_params}\")\n",
    "\n",
    "async def main():\n",
    "    \"\"\"\n",
    "    Main entry point for running the distributed training simulation.\n",
    "    \"\"\"\n",
    "    # Run distributed training with 3 workers, 5 parameters, and 10 iterations per worker\n",
    "    await distributed_training(num_workers=3, param_size=5, num_iterations=10)\n",
    "\n",
    "# Start the asynchronous event loop and execute the main function\n",
    "asyncio.run(main())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
